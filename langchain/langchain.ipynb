{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "owWp6tH4CuyP"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain langchain-neo4j langchain-openai langchain-mcp-adapters mcp-neo4j-cypher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXT_WL0ZqAca"
      },
      "source": [
        "The **LangChain framework for Python** is a toolkit for building applications powered by large language models. It provides composable chains and agents, a vast integration ecosystem, memory and retrieval systems, and production essentials like callbacks, tracing, and evaluation tools.\n",
        "\n",
        "In this notebook, we'll build a company research agent that queries a Neo4j graph database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "up-PVir6DR2A"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from langchain.tools import tool\n",
        "from langchain.agents import create_agent\n",
        "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EON_OelfqGXG"
      },
      "source": [
        "LangChain integrates with virtually every major LLM provider like OpenAI, Anthropic, Google, Cohere, Mistral, AWS Bedrock, Azure, and many more. This makes it easy to swap models or run comparisons without rewriting your application logic.\n",
        "\n",
        "In this example, we'll use OpenAI as our LLM provider, specifically GPT-5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY9l5jrmDSYA",
        "outputId": "2279b1c7-fc52-4f0f-edff-962e2ccf968d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MFAuqmMkqjQ-"
      },
      "outputs": [],
      "source": [
        "model =  ChatOpenAI(model=\"gpt-5.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDKLPaYgqnQs"
      },
      "source": [
        "For this example, we'll use the companies database from the Neo4j demo server, which contains organizations, people, investors, and news articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dHSYEmMBDdMs"
      },
      "outputs": [],
      "source": [
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://demo.neo4jlabs.com\"\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"companies\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"companies\"\n",
        "os.environ[\"NEO4J_DATABASE\"] = \"companies\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6slNxMSukE7I"
      },
      "source": [
        "## MCP Neo4j Cypher\n",
        "\n",
        "We'll start by using the `mcp-neo4j-cypher` to extend the agent with Neo4j tools. This MCP server provides the agent with capabilities to read the graph schema and execute Cypher queries, enabling it to fetch and analyze data directly from the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3j0zcxLITvc"
      },
      "outputs": [],
      "source": [
        "cypher_mcp_config = {\n",
        "    \"neo4j-database\": {\n",
        "        \"transport\": \"stdio\",\n",
        "        \"command\": \"uvx\",\n",
        "        \"args\": [\"mcp-neo4j-cypher\"],\n",
        "        \"env\": {\n",
        "            \"NEO4J_URI\": os.environ[\"NEO4J_URI\"],\n",
        "            \"NEO4J_USERNAME\": os.environ[\"NEO4J_USERNAME\"],\n",
        "            \"NEO4J_PASSWORD\": os.environ[\"NEO4J_PASSWORD\"],\n",
        "            \"NEO4J_DATABASE\": os.environ[\"NEO4J_DATABASE\"],\n",
        "        },\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP6ndDyOrOpD"
      },
      "source": [
        "**Google Colab users only:** Run the following cell to start the MCP server with HTTP transport. This workaround is necessary because Google Colab doesn't support the default stdio transport method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5RuWfDuEd2r"
      },
      "outputs": [],
      "source": [
        "# Google Colab Setup: MCP Server for Neo4j\n",
        "#\n",
        "# This cell launches the Neo4j MCP (Model Context Protocol) server as a background process.\n",
        "# MCP provides a standardized way for LLMs to interact with external tools and databases.\n",
        "#\n",
        "# The server exposes Cypher query capabilities over HTTP, allowing our LangChain agent\n",
        "# to read schema information and execute queries against the Neo4j database.\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_server():\n",
        "    subprocess.run([\n",
        "        \"mcp-neo4j-cypher\",\n",
        "        \"--server-port\", \"8000\",\n",
        "        \"--db-url\", os.environ[\"NEO4J_URI\"],\n",
        "        \"--username\", os.environ[\"NEO4J_USERNAME\"],\n",
        "        \"--password\", os.environ[\"NEO4J_PASSWORD\"],\n",
        "        \"--database\", os.environ[\"NEO4J_DATABASE\"],\n",
        "        \"--transport\", \"http\"\n",
        "    ])\n",
        "\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "time.sleep(10)\n",
        "\n",
        "cypher_mcp_config = {\"neo4j-database\": {\n",
        "            \"url\": \"http://localhost:8000/mcp\",\n",
        "            \"transport\": \"streamable_http\"\n",
        "        }}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxnFDa2QrpRp"
      },
      "source": [
        "With the MCP server running, we initialize a client to connect to it and retrieve the available tools. These tools will allow our agent to query the Neo4j database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zst3ezlEV-h"
      },
      "outputs": [],
      "source": [
        "# If there is an error, just rerun as the MCP server might not be running yet\n",
        "\n",
        "client = MultiServerMCPClient(cypher_mcp_config)\n",
        "mcp_tools = await client.get_tools()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyDIXIb5r9f6"
      },
      "source": [
        "We define a system prompt that instructs the agent on its role and capabilities. The `create_agent` function constructs a **ReAct-style** agent that follows a reasoning loop: it observes the current state, decides which tool to use (if any), executes the tool, and incorporates the result into its next step. This architecture allows the agent to chain multiple tool calls together to answer complex questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtM9W1G4JEMp"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant with access to a Neo4j graph database containing company data.\n",
        "Use the available tools to query the database and answer questions.\n",
        "\"\"\"\n",
        "\n",
        "agent = create_agent(model, mcp_tools, system_prompt=system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG4visKXsIW5"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnoEpj43JhVh",
        "outputId": "6568381d-6880-4d7e-d391-f39f71f7d940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "How many people are in the database?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  read_neo4j_cypher (call_phJqd8gRMWViZnthEfBSS5FL)\n",
            " Call ID: call_phJqd8gRMWViZnthEfBSS5FL\n",
            "  Args:\n",
            "    query: MATCH (p:Person) RETURN count(p) AS peopleCount\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: read_neo4j_cypher\n",
            "\n",
            "[{'type': 'text', 'text': '[{\"peopleCount\": 8064}]', 'id': 'lc_2598b242-6244-40aa-bff7-3d59537557f4'}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "There are 8,064 people in the database.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"How many people are in the database?\"\n",
        "\n",
        "async for event in agent.astream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ4FhFgcLGOB"
      },
      "source": [
        "# Custom tools\n",
        "\n",
        "Beyond using existing MCP servers, you can also implement your own custom tools and add them directly to the agent. This allows you to create specialized functionality tailored to your specific use case. Custom tools can be implemented using the `@tool` decorator, which turns any function into a tool the agent can invoke.\n",
        "\n",
        "Here, we use `Neo4jGraph` from the `langchain-neo4j` package, a direct integration in the LangChain ecosystem, to establish a connection to our database and build a tool that queries investment relationships, giving you more control over the query logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "paDfdYT3h-gD"
      },
      "outputs": [],
      "source": [
        "neo4j_graph = Neo4jGraph()\n",
        "\n",
        "@tool\n",
        "async def get_investments(company: str) -> str:\n",
        "    \"\"\"Returns the investments by a company by name. Returns list of investment ids, names and types.\"\"\"\n",
        "    try:\n",
        "        results = neo4j_graph.query(\"\"\"\n",
        "            MATCH (o:Organization)-[:HAS_INVESTOR]->(i)\n",
        "            WHERE o.name = $company\n",
        "            RETURN i.id as id, i.name as name, head(labels(i)) as type\n",
        "        \"\"\", {\"company\": company})\n",
        "        return json.dumps(results, indent=2)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error fetching investments: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xmqQhw_uEHr"
      },
      "source": [
        "The `langchain-neo4j` package also provides `Neo4jVector`, a vector store integration that enables semantic search over your graph data. Here, we connect to an existing vector index and create a tool that uses OpenAI embeddings to search for relevant news chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dm-agJP-ELTp"
      },
      "outputs": [],
      "source": [
        "vector_store = Neo4jVector.from_existing_index(\n",
        "    OpenAIEmbeddings(),\n",
        "    index_name=\"news\",\n",
        "    node_label=\"Chunk\",\n",
        "    retrieval_query=\"\"\"\n",
        "    MATCH (node)<-[:HAS_CHUNK]-(a:Article)\n",
        "    RETURN node.text AS text, score, {date: a.date} AS metadata\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "@tool\n",
        "def retrieve_news(query: str) -> str:\n",
        "    \"\"\"Search for relevant news articles. Returns up to 5 articles with their source metadata and content.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=5)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmqriExub9a"
      },
      "source": [
        "We combine the MCP tools with our custom tools into a single list and create a new agent with access to all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9OD6_4aDC9vq"
      },
      "outputs": [],
      "source": [
        "custom_tools = mcp_tools + [get_investments, retrieve_news]\n",
        "# If desired, specify custom instructions\n",
        "prompt = (\n",
        "    \"You are a helpful assistant with access to a Neo4j graph database containing company data. Use the available tools to query the database and answer questions.\"\n",
        ")\n",
        "custom_agent = create_agent(model, custom_tools, system_prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZAdr9MulOX"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMWEcNpki4ai",
        "outputId": "c649044d-9387-4cf8-daf1-6b7a704882a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Which companies did Google invest in?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_investments (call_OjjAOjy1pivFZYLDKkOM616B)\n",
            " Call ID: call_OjjAOjy1pivFZYLDKkOM616B\n",
            "  Args:\n",
            "    company: Google\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_investments\n",
            "\n",
            "[\n",
            "  {\n",
            "    \"id\": \"ELsv5bECSOiWG_Uhf_txI2w\",\n",
            "    \"name\": \"Ionic Security\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EUkm62r-bMOidNtPjTkdVvg\",\n",
            "    \"name\": \"Avere Systems\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EX-RLztfkOFqTLoM6xIVnlg\",\n",
            "    \"name\": \"FlexiDAO\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EtqXbQ9LaMGq8om4dhYY0Fw\",\n",
            "    \"name\": \"Cloudflare\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EWIvDLNCSMCCBYUyz0oFPVQ\",\n",
            "    \"name\": \"Trifacta\",\n",
            "    \"type\": \"Organization\"\n",
            "  }\n",
            "]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "According to the database, Google has invested in the following companies:\n",
            "\n",
            "1. Ionic Security  \n",
            "2. Avere Systems  \n",
            "3. FlexiDAO  \n",
            "4. Cloudflare  \n",
            "5. Trifacta  \n",
            "\n",
            "These are the investments currently recorded in the graph; it may not be an exhaustive list of all real-world Google investments.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Which companies did Google invest in?\"\n",
        "\n",
        "async for event in custom_agent.astream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "In this notebook, we built a company research agent using LangChain with Neo4j:\n",
        "1. **MCP Integration** — Connected to Neo4j using the `mcp-neo4j-cypher` server via `langchain-mcp-adapters` for schema reading and Cypher queries\n",
        "2. **ReAct Agent** — Created a reasoning agent with `create_agent` that chains tool calls to answer complex questions\n",
        "3. **Custom Tools** — Built specialized tools using the `@tool` decorator with direct `Neo4jGraph` or `Neo4jVector` integrations\n",
        "\n",
        "The LangChain framework makes it straightforward to combine MCP servers with custom tools, swap LLM providers, and build composable agent workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTNbWK_kLU4"
      },
      "source": [
        "## ToDo\n",
        "\n",
        "* [ ] Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QZiuGifYoa4I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
