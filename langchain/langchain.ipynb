{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owWp6tH4CuyP"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain langchain-neo4j langchain-openai langchain-mcp-adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXT_WL0ZqAca"
      },
      "source": [
        "The **LangChain framework for Python** is a toolkit for building applications powered by large language models. It provides composable chains and agents, a vast integration ecosystem, memory and retrieval systems, and production essentials like callbacks, tracing, and evaluation tools.\n",
        "\n",
        "In this notebook, we'll build a company research agent that queries a Neo4j graph database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "up-PVir6DR2A"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from langchain_neo4j import Neo4jGraph, Neo4jVector, AsyncNeo4jSaver\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EON_OelfqGXG"
      },
      "source": [
        "LangChain integrates with virtually every major LLM provider like OpenAI, Anthropic, Google, Cohere, Mistral, AWS Bedrock, Azure, and many more. This makes it easy to swap models or run comparisons without rewriting your application logic.\n",
        "\n",
        "In this example, we'll use OpenAI as our LLM provider, specifically **GPT-5.1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY9l5jrmDSYA",
        "outputId": "9f9f0b34-8cf2-435e-c60e-ed4106678eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MFAuqmMkqjQ-"
      },
      "outputs": [],
      "source": [
        "model =  ChatOpenAI(model=\"gpt-5.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6slNxMSukE7I"
      },
      "source": [
        "## Neo4j MCP Server\n",
        "\n",
        "We'll start by using the official [Neo4j MCP Server](https://github.com/neo4j/mcp) to extend the agent with Neo4j tools. This MCP server provides the agent with capabilities to read the graph schema and execute Cypher queries, enabling it to fetch and analyze data directly from the database.\n",
        "\n",
        "The following code installs the latest version on Google Colab and similar Linux-based systems. For other operating systems, please consult the [official installation documentation](https://neo4j.com/docs/mcp/current/installation/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I5RuWfDuEd2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a5539a-9afe-4949-a448-37e079b7c5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest version: v1.2.0\n",
            "neo4j-mcp version: v1.2.0\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Get latest release info from GitHub API\n",
        "release = requests.get(\"https://api.github.com/repos/neo4j/mcp/releases/latest\").json()\n",
        "version = release[\"tag_name\"]\n",
        "print(f\"Latest version: {version}\")\n",
        "\n",
        "# Download the latest Linux binary\n",
        "!wget -q https://github.com/neo4j/mcp/releases/download/{version}/neo4j-mcp_Linux_x86_64.tar.gz\n",
        "\n",
        "# Extract\n",
        "!tar -xzf neo4j-mcp_Linux_x86_64.tar.gz\n",
        "\n",
        "# Make executable\n",
        "!chmod +x neo4j-mcp\n",
        "\n",
        "# Cleanup\n",
        "!rm neo4j-mcp_Linux_x86_64.tar.gz\n",
        "\n",
        "# Move\n",
        "!mv neo4j-mcp /usr/local/bin/\n",
        "\n",
        "# Verify installation\n",
        "!neo4j-mcp -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDKLPaYgqnQs"
      },
      "source": [
        "For this example, we'll use the companies database from the Neo4j demo server, which contains organizations, people, investors, and news articles.\n",
        "\n",
        "For HTTP transport, you only need to set the `NEO4J_URI` and optionally `NEO4J_DATABASE` (if connecting to a specific database)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dHSYEmMBDdMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16c0419-c82a-455d-95ce-0cccce1a9957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['neo4j-mcp']>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.environ[\"NEO4J_URI\"] = \"neo4j+s://demo.neo4jlabs.com\"\n",
        "os.environ[\"NEO4J_DATABASE\"] = \"companies\"\n",
        "os.environ[\"NEO4J_MCP_TRANSPORT\"] = \"http\"\n",
        "\n",
        "# Run the server in the background\n",
        "import subprocess\n",
        "subprocess.Popen([\"neo4j-mcp\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credentials are passed via the `Authorization` header using Basic authentication. The HTTP transport listens on port 80 by default, so the MCP endpoint is available at `http://localhost:80/mcp`."
      ],
      "metadata": {
        "id": "R2Qc4PpLZ9bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credentials are passed via bearer auth\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"companies\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"companies\"\n",
        "\n",
        "credentials = base64.b64encode(f\"{os.environ[\"NEO4J_USERNAME\"]}:{os.environ[\"NEO4J_PASSWORD\"]}\".encode()).decode()\n",
        "\n",
        "cypher_mcp_config = {\n",
        "    \"neo4j-database\": {\n",
        "        \"transport\": \"http\",\n",
        "        \"url\": \"http://localhost:80/mcp\",\n",
        "        \"headers\": {\n",
        "            \"Authorization\": f\"Basic {credentials}\"\n",
        "        },\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "cYIr7QhnWwpA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxnFDa2QrpRp"
      },
      "source": [
        "With the MCP server running, we initialize a client to connect to it and retrieve the available tools. These tools will allow our agent to query the Neo4j database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2Zst3ezlEV-h"
      },
      "outputs": [],
      "source": [
        "# If there is an error, just rerun as the MCP server might not be running yet\n",
        "\n",
        "client = MultiServerMCPClient(cypher_mcp_config)\n",
        "mcp_tools = await client.get_tools()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporary Workaround\n",
        "\n",
        "The `get-schema` and `list-gds-procedures` tools return an invalid `args_schema` format (`{'type': 'object'}` dict instead of a Pydantic `BaseModel` class). This causes validation errors when the tools are invoked.\n",
        "\n",
        "The following workaround replaces the malformed schema in `langchain` with a proper empty Pydantic model until the upstream MCP adapter is fixed."
      ],
      "metadata": {
        "id": "2uqk6L0INdz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty schema for tools with no arguments\n",
        "class EmptySchema(BaseModel):\n",
        "    \"\"\"Schema for tools that take no arguments.\"\"\"\n",
        "    pass\n",
        "\n",
        "def fix_empty_args_schema(tools: list[StructuredTool]) -> list[StructuredTool]:\n",
        "    \"\"\"Fix tools that have dict-based empty schemas instead of proper Pydantic models.\"\"\"\n",
        "    fixed_tools = []\n",
        "    for tool in tools:\n",
        "        # Check if args_schema is a dict (incorrect) instead of a BaseModel class\n",
        "        if isinstance(tool.args_schema, dict) and tool.args_schema == {'type': 'object'}:\n",
        "            # Create a new tool with the proper empty schema\n",
        "            fixed_tool = StructuredTool(\n",
        "                name=tool.name,\n",
        "                description=tool.description,\n",
        "                args_schema=EmptySchema,\n",
        "                metadata=tool.metadata,\n",
        "                response_format=tool.response_format,\n",
        "                coroutine=tool.coroutine,\n",
        "                func=tool.func,\n",
        "            )\n",
        "            fixed_tools.append(fixed_tool)\n",
        "        else:\n",
        "            fixed_tools.append(tool)\n",
        "    return fixed_tools\n",
        "\n",
        "# Usage:\n",
        "mcp_tools = fix_empty_args_schema(mcp_tools)"
      ],
      "metadata": {
        "id": "hLAyNEgubuZG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyDIXIb5r9f6"
      },
      "source": [
        "We define a system prompt that instructs the agent on its role and capabilities. The `create_agent` function constructs a **ReAct-style** agent that follows a reasoning loop: it observes the current state, decides which tool to use (if any), executes the tool, and incorporates the result into its next step. This architecture allows the agent to chain multiple tool calls together to answer complex questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MtM9W1G4JEMp"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant with access to a Neo4j graph database containing company data.\n",
        "Use the available tools to query the database and answer questions.\n",
        "\"\"\"\n",
        "\n",
        "agent = create_agent(model, mcp_tools, system_prompt=system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG4visKXsIW5"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnoEpj43JhVh",
        "outputId": "a3dc99c9-f8db-4767-f2ac-c379d87bdb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "How many people are in the database?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  read-cypher (call_yT5PMB85MOU0WSpjb73EsMCD)\n",
            " Call ID: call_yT5PMB85MOU0WSpjb73EsMCD\n",
            "  Args:\n",
            "    query: MATCH (p:Person) RETURN count(p) AS people\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: read-cypher\n",
            "\n",
            "[{'type': 'text', 'text': '[\\n  {\\n    \"people\": 8064\\n  }\\n]', 'id': 'lc_28e9bb93-3c45-4523-b027-15d31e04abc3'}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "There are 8,064 people in the database.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"How many people are in the database?\"\n",
        "\n",
        "async for event in agent.astream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ4FhFgcLGOB"
      },
      "source": [
        "# Custom tools\n",
        "\n",
        "Beyond using existing MCP servers, you can also implement your own custom tools and add them directly to the agent. This allows you to create specialized functionality tailored to your specific use case. Custom tools can be implemented using the `@tool` decorator, which turns any function into a tool the agent can invoke.\n",
        "\n",
        "Here, we use `Neo4jGraph` from the `langchain-neo4j` package, a direct integration in the LangChain ecosystem, to establish a connection to our database and build a tool that queries investment relationships, giving you more control over the query logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "paDfdYT3h-gD"
      },
      "outputs": [],
      "source": [
        "neo4j_graph = Neo4jGraph(refresh_schema=False)\n",
        "\n",
        "@tool\n",
        "async def get_investments(company: str) -> str:\n",
        "    \"\"\"Returns the investments by a company by name. Returns list of investment ids, names and types.\"\"\"\n",
        "    try:\n",
        "        results = neo4j_graph.query(\"\"\"\n",
        "            MATCH (o:Organization)-[:HAS_INVESTOR]->(i)\n",
        "            WHERE o.name = $company\n",
        "            RETURN i.id as id, i.name as name, head(labels(i)) as type\n",
        "        \"\"\", {\"company\": company})\n",
        "        return json.dumps(results, indent=2)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error fetching investments: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xmqQhw_uEHr"
      },
      "source": [
        "The `langchain-neo4j` package also provides `Neo4jVector`, a vector store integration that enables semantic search over your graph data. Here, we connect to an existing vector index and create a tool that uses OpenAI embeddings to search for relevant news chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dm-agJP-ELTp"
      },
      "outputs": [],
      "source": [
        "vector_store = Neo4jVector.from_existing_index(\n",
        "    OpenAIEmbeddings(),\n",
        "    index_name=\"news\",\n",
        "    node_label=\"Chunk\",\n",
        "    retrieval_query=\"\"\"\n",
        "    MATCH (node)<-[:HAS_CHUNK]-(a:Article)\n",
        "    RETURN node.text AS text, score, {date: a.date} AS metadata\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "@tool\n",
        "def retrieve_news(query: str) -> str:\n",
        "    \"\"\"Search for relevant news articles. Returns up to 5 articles with their source metadata and content.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=5)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmqriExub9a"
      },
      "source": [
        "We combine the MCP tools with our custom tools into a single list and create a new agent with access to all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9OD6_4aDC9vq"
      },
      "outputs": [],
      "source": [
        "custom_tools = mcp_tools + [get_investments, retrieve_news]\n",
        "# If desired, specify custom instructions\n",
        "prompt = (\n",
        "    \"You are a helpful assistant with access to a Neo4j graph database containing company data. Use the available tools to query the database and answer questions.\"\n",
        ")\n",
        "custom_agent = create_agent(model, custom_tools, system_prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZAdr9MulOX"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMWEcNpki4ai",
        "outputId": "0d869569-bc50-4654-d2f5-9b37ff7fcb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Which companies did Google invest in?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_investments (call_twCNXWBXezSYoJJH6IouLm6p)\n",
            " Call ID: call_twCNXWBXezSYoJJH6IouLm6p\n",
            "  Args:\n",
            "    company: Google\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_investments\n",
            "\n",
            "[\n",
            "  {\n",
            "    \"id\": \"ELsv5bECSOiWG_Uhf_txI2w\",\n",
            "    \"name\": \"Ionic Security\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EUkm62r-bMOidNtPjTkdVvg\",\n",
            "    \"name\": \"Avere Systems\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EX-RLztfkOFqTLoM6xIVnlg\",\n",
            "    \"name\": \"FlexiDAO\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EtqXbQ9LaMGq8om4dhYY0Fw\",\n",
            "    \"name\": \"Cloudflare\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EWIvDLNCSMCCBYUyz0oFPVQ\",\n",
            "    \"name\": \"Trifacta\",\n",
            "    \"type\": \"Organization\"\n",
            "  }\n",
            "]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Google has invested in the following companies in this dataset:\n",
            "\n",
            "1. Ionic Security  \n",
            "2. Avere Systems  \n",
            "3. FlexiDAO  \n",
            "4. Cloudflare  \n",
            "5. Trifacta  \n",
            "\n",
            "If you’d like, I can look up more details (e.g., what these companies do or their other investors).\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Which companies did Google invest in?\"\n",
        "\n",
        "async for event in custom_agent.astream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTNbWK_kLU4"
      },
      "source": [
        "## Short-term memory - Checkpoint saver\n",
        "\n",
        "LangChain agents are stateless by default—each invocation starts fresh with no memory of previous interactions. For multi-turn conversations, you need a **checkpointer** to persist the agent's state between calls.\n",
        "\n",
        "The `langchain-neo4j` package provides `Neo4jSaver` and `AsyncNeo4jSaver`, which store conversation checkpoints directly in Neo4j. This enables:\n",
        "- **Conversation continuity** across multiple interactions\n",
        "- **Session recovery** if the application restarts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_NEO4J_URI = \"bolt://44.203.8.239:7687\"\n",
        "CHECKPOINT_NEO4J_USERNAME = \"neo4j\"\n",
        "CHECKPOINT_NEO4J_PASSWORD = \"reenlistment-quarts-battleship\"\n",
        "CHECKPOINT_NEO4J_DATABASE = \"neo4j\"\n",
        "\n",
        "async with await AsyncNeo4jSaver.from_conn_string(\n",
        "    uri=CHECKPOINT_NEO4J_URI,\n",
        "    user=CHECKPOINT_NEO4J_USERNAME,\n",
        "    password=CHECKPOINT_NEO4J_PASSWORD,\n",
        "    database=CHECKPOINT_NEO4J_DATABASE\n",
        ") as checkpointer:\n",
        "    await checkpointer.setup()\n",
        "\n",
        "    agent = create_agent(\n",
        "        model,\n",
        "        custom_tools,\n",
        "        system_prompt=prompt,\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "\n",
        "    async for event in agent.astream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "        {\"configurable\": {\"thread_id\": \"11\"}},\n",
        "        stream_mode=\"values\",\n",
        "    ):\n",
        "        event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wYCu4yFOsj5",
        "outputId": "90135120-5c7c-4e2f-95e1-2d831f67143e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Which companies did Google invest in?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_investments (call_iU3Nm1AwB3uJMRGVGx1VRsOV)\n",
            " Call ID: call_iU3Nm1AwB3uJMRGVGx1VRsOV\n",
            "  Args:\n",
            "    company: Google\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_investments\n",
            "\n",
            "[\n",
            "  {\n",
            "    \"id\": \"ELsv5bECSOiWG_Uhf_txI2w\",\n",
            "    \"name\": \"Ionic Security\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EUkm62r-bMOidNtPjTkdVvg\",\n",
            "    \"name\": \"Avere Systems\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EX-RLztfkOFqTLoM6xIVnlg\",\n",
            "    \"name\": \"FlexiDAO\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EtqXbQ9LaMGq8om4dhYY0Fw\",\n",
            "    \"name\": \"Cloudflare\",\n",
            "    \"type\": \"Organization\"\n",
            "  },\n",
            "  {\n",
            "    \"id\": \"EWIvDLNCSMCCBYUyz0oFPVQ\",\n",
            "    \"name\": \"Trifacta\",\n",
            "    \"type\": \"Organization\"\n",
            "  }\n",
            "]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on the data available here, Google has invested in the following companies:\n",
            "\n",
            "1. Ionic Security  \n",
            "2. Avere Systems  \n",
            "3. FlexiDAO  \n",
            "4. Cloudflare  \n",
            "5. Trifacta  \n",
            "\n",
            "If you want, I can add brief descriptions of what each of these companies does.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqFRMrpOV96T"
      },
      "source": [
        "\n",
        "The `thread_id` in the config uniquely identifies a conversation. Messages with the same thread ID share context, while different thread IDs maintain separate conversation histories.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we built a company research agent using LangChain with Neo4j:\n",
        "\n",
        "1. **MCP Integration** — Connected to Neo4j using the official Neo4j MCP server for schema reading and Cypher queries\n",
        "2. **ReAct Agent** — Created a reasoning agent with `create_agent` that chains tool calls to answer complex questions\n",
        "3. **Custom Tools** — Built specialized tools using the `@tool` decorator with direct `Neo4jGraph` or `Neo4jVector` integrations\n",
        "4. **Short-term Memory** — Added conversation persistence with `AsyncNeo4jSaver` to enable multi-turn interactions\n",
        "\n",
        "The LangChain framework makes it straightforward to combine MCP servers with custom tools, swap LLM providers, persist conversation state, and build composable agent workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiuGifYoa4I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}